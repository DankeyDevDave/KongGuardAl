version: '3.8'

networks:
  kong-net:
    driver: bridge

volumes:
  kong-datastore:
  konga-datastore:
  kong-plugins:

services:
  # PostgreSQL Database for Kong
  kong-database:
    image: postgres:13
    container_name: kong-database
    restart: on-failure
    networks:
      - kong-net
    volumes:
      - kong-datastore:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: kong
      POSTGRES_PASSWORD: kongpass
      POSTGRES_DB: kong
    ports:
      - "15432:5432"  # Non-standard port to avoid conflicts
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "kong"]
      interval: 30s
      timeout: 30s
      retries: 3

  # Kong Database Migration
  kong-migrations:
    image: kong:3.8.0
    container_name: kong-migrations
    command: kong migrations bootstrap
    depends_on:
      kong-database:
        condition: service_healthy
    networks:
      - kong-net
    restart: on-failure
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_PORT: 5432
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: kongpass
      KONG_PG_DATABASE: kong

  # Kong Guard AI - Threat Analysis Service
  ai-service:
    build:
      context: ./ai-service
      dockerfile: Dockerfile
    container_name: kong-guard-ai-service
    networks:
      - kong-net
    ports:
      - "18002:8000"  # Non-standard port to avoid conflicts
    environment:
      # Choose your AI provider: openai, groq, gemini, ollama
      AI_PROVIDER: ${AI_PROVIDER:-gemini}
      
      # API Keys - Set the one you're using
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      GROQ_API_KEY: ${GROQ_API_KEY:-}
      GEMINI_API_KEY: ${GEMINI_API_KEY:-}
      
      # For Ollama (local LLM)
      OLLAMA_URL: http://ollama:11434
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: on-failure

  # Ollama (Optional - for local LLM)
  # Uncomment to use local models
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   networks:
  #     - kong-net
  #   volumes:
  #     - ./ollama-models:/root/.ollama
  #   ports:
  #     - "11434:11434"
  #   restart: on-failure

  # Kong Gateway with AI Plugin
  kong:
    image: kong:3.8.0
    container_name: kong-gateway
    user: "${KONG_USER:-kong}"
    depends_on:
      kong-database:
        condition: service_healthy
      kong-migrations:
        condition: service_completed_successfully
      ai-service:
        condition: service_healthy
    networks:
      - kong-net
    ports:
      - "18000:8000/tcp"    # Kong proxy port (HTTP)
      - "18443:8443/tcp"    # Kong proxy port (HTTPS)
      - "18001:8001/tcp"    # Kong admin API (HTTP)
      - "18444:8444/tcp"    # Kong admin API (HTTPS)
    volumes:
      - ./kong-plugin/kong/plugins:/usr/local/share/lua/5.1/kong/plugins
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_PORT: 5432
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: kongpass
      KONG_PG_DATABASE: kong
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: "0.0.0.0:8001, 0.0.0.0:8444 ssl"
      KONG_PROXY_LISTEN: "0.0.0.0:8000, 0.0.0.0:8443 ssl"
      KONG_PLUGINS: "kong-guard-ai"
      KONG_LOG_LEVEL: debug
      
      # AI Configuration for Plugin
      AI_SERVICE_URL: http://ai-service:8000
      AI_PROVIDER: ${AI_PROVIDER:-gemini}
      AI_API_KEY: ${GEMINI_API_KEY:-}
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 10s
      timeout: 10s
      retries: 10
    restart: on-failure:5

  # Demo API Service for Testing
  demo-api:
    image: kennethreitz/httpbin
    container_name: demo-api
    networks:
      - kong-net
    ports:
      - "18085:80"  # Non-standard port
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/status/200"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis for plugin state management
  redis:
    image: redis:7-alpine
    container_name: kong-redis
    networks:
      - kong-net
    ports:
      - "16379:6379"
    command: redis-server --appendonly yes
    volumes:
      - ./redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Konga Database
  konga-database:
    image: postgres:11
    container_name: konga-database
    restart: on-failure
    networks:
      - kong-net
    environment:
      POSTGRES_USER: konga
      POSTGRES_PASSWORD: kongapass
      POSTGRES_DB: konga
    volumes:
      - konga-datastore:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "konga"]
      interval: 30s
      timeout: 30s
      retries: 3

  # Konga DB prepare
  konga-prepare:
    image: pantsel/konga:latest
    container_name: konga-prepare
    command: "-c prepare -a postgres -u postgresql://konga:kongapass@konga-database:5432/konga"
    networks:
      - kong-net
    restart: on-failure
    depends_on:
      konga-database:
        condition: service_healthy
    environment:
      DB_ADAPTER: postgres
      DB_HOST: konga-database
      DB_PORT: 5432
      DB_USER: konga
      DB_PASSWORD: kongapass
      DB_DATABASE: konga

  # Konga - Kong Admin GUI
  konga:
    image: pantsel/konga:latest
    container_name: konga
    restart: on-failure
    networks:
      - kong-net
    environment:
      NODE_ENV: production
      TOKEN_SECRET: km1GUr4RkcQD7DewhJPNXrCuZwcKmqjb
      DB_ADAPTER: postgres
      DB_HOST: konga-database
      DB_PORT: 5432
      DB_USER: konga
      DB_PASSWORD: kongapass
      DB_DATABASE: konga
    ports:
      - "1337:1337"
    depends_on:
      konga-database:
        condition: service_healthy
      konga-prepare:
        condition: service_completed_successfully
      kong:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:1337"]
      interval: 30s
      timeout: 10s
      retries: 5